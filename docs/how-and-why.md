If you want to get started head to the [edge-lab-infra](https://github.com/ashmantis1/edge-lab-infra) repository. The README will outline most of the important steps to actually get started.

## How it all Works

### Sushy Tools 
This project is  made possible through the use of the [Sushy Tools Redfish Emulator](https://docs.openstack.org/sushy/latest/). This emulator provides a Redfish Endpoint, which is required by Ironic and therefore Metal3, in order to control power state and provision nodes. However, the emulator does not have any support for baremetal power control (because why would it lol) out of the box. To solve this "issue", I wrote a [driver for Sushy Tools](https://github.com/ashmantis1/sushy-tools-baremetal). This driver adds the ability to control two specific power control methods: 

- Intel AMT
- TPLink Tapo P100 Smart Plugs

Intel AMT is an obvious solution, as it is a BMC commonly available on consumer machines (technically it's supported by a third party Ironic driver as well, but it isn't supported by Metal3). However when planning this project, I also bought some machines which did not include AMT or any other form of remote power control. While options such as PiKVM seemed promising, they did not prove to be particularly cost effective. Instead, I went out and bought some WiFi smart plugs, which could be controlled via a local API (making sure to buy ones which did not require an internet connection). At about $15 each, these seemed to be a cheap and easy way to provide BMC. 

I still needed a way for Metal3 to interact with the "BMC" options I had come up with, so I decided to write a driver for Sushy Tools. This driver is based on the "Fake Driver" provided by Sushy Tools. TheFake Driver provides all the same endpoints as a real Redfish device, and out of the box will respond to all requests (including POST and PATCH). It does this by storing and updating data in an SQLite database. I created a copy of this file, which would act as the base for the driver. I then added config options for the new driver, to allow it to be selected for use. To control the Tapo smart plugs, I used the [PyP100 python library](https://github.com/fishbigger/TapoP100/blob/main/PyP100/PyP100.py). This meant that all I needed to do was replace the Fake Driver's power state functionality, with function calls to this library (and the equivelant for AMT), pulling from the configuration file to get IP address, and authentication information for the Tapo devices. 

It's important to remember Ironic only really* cares about three things: 

- Power State
- Power On
- Power Off

However, Ironic does try to get a lot of other information from the Redfish endpoint during host inspection. Luckily as I mentioned earlier, Sushy Tools Fake Driver provides every endpoint. This meant I could leave most of the "unnecessary" endpoints as they were, because even though the results they would return might not be accurate, it wouldn't affect the functionality with Metal3.

With all the important endpoints implemented (all very similar for AMT but a little simpler), I started testing the functionality of the emulator. Initially, everything seemed promising. I tested doing basic requests to control power state using curl, which worked perfectly. I then created a BareMetalHost resource in my kubernetes cluster, pointing it to the Redfish emulator in the BMC field. This worked atfirst. The node would spin to life and boot using the PXE server provider by the Ironic deployment (I was pretty excited when I first saw that). However, occasionally there would be no response when Ironicwould request the node to power on. After a little bit of troubleshooting, I realised that the Tapo smart plugs were essentially being DOS'd by Ironic. Because Ironic tries to get the nodes power state every few seconds, the little microcontrontrollers in the smart plugs couldn't keep up, and stopped responding to requests, which caused Ironic to enter error states. 

To fix this issue, I used the afformentioned SQLite database to cache the current power state of a node. This means that when a request to get the current power state is made by Ironic, the driver will respond with the same power state for about 30 seconds after the last state was pulled from the plugs, instead of getting a new one every time from the device. After that period elapses, any future request will call the plugs API directly, and start the countdown again. After this fix was implemented, the node was now (relatively) consistently turning on and off as Ironic requested. 

With BMC out of the way, now came deploying the cluster...

* Ironic probably cares a bit about other things, but I don't


### Metal3 and Cluster API 
For me, Metal3 was not an obvious choice for this project, mostly due to the hard requirement of BMC. Initially I intended to use Talos, with something like Poseidon Matchbox, Foreman or Sidero Metal. All these options are perfectly valid, and may have provided a quicker path to a kubernetes cluster, however I decided that I didn't want anything less than fully automatic and declaritive provisioning and deprovisioning of clusters, which no other option would have provided as a full package. This is because Metal3 integrates directly with ClusterAPI, meaning that kubernetes clusters, and all their components (controlplane, workers, loadbalancers, etc.) can be defined as kubernetes resources. The ultimate result being a kubernetes cluster can be fully provisioned from the ground up with a single `kubectl apply`, or better yet, a push to a git repository. 

The process to get Metal3 working was relatively straight forward. Each host can be defined as a BareMetalHost custom resource, which includes specifications such as the BMC (in this case the Sushy Tool endpoint), and MAC address, among other things. Once the host is defined, the manifest can be applied, and the machine should spin to life. The first step before a node can be used is Ironic's inspection phase. This is where the Ironic Inspection Agent image is loaded into a ramdisk on PXE boot, and all the data about the node is reported back to Ironic, and in turn the Baremetal Operator (Metal3's operator which controls most of the functionality). That data is then available under the status object on the BareMetalHost. After this process is completed, the node will become "available" and can be consumed by ClusterAPI. 

Initially I didn't actually intend to use ClusterAPI either. After I decided on Metal3 as my provisioning service, I was just going to deploy the nodes and automate the cluster deployment seperately. However, after reading the docs for Metal3, I couldn't help but give it a go. ClusterAPI works by utilising a few different providers. The Infrastructure provider, in this case Metal3, which is used to provision the cluster's infrastructure, the Control Plane provider (who's use is probably obvious), and the Bootstrap provider, which is used to provision worker machines. Conveniently, ClusterAPI ships with KubeADM based Bootstrap and Control plane providers, which provides enough flexibility to deploy the cluster in whatever way you would like. 

I first tested a single node kubeadm deployment, which was as simple as generating the cluster config using `clusterctl`, and modifying the deployment to set replicas for the KubeadmControlPlane to 1, and the MachineDeployment (worker nodes) to 0. This seemed to work well, as I saw the node spin to life and become provisioned, however the control plane never became available. As it turned out, this was because the kubeadm provider in ClusterAPI doesn't actually ship with a Loadbalancer/Virtual IP solution for the control plane, although initially I didn't realise this. After a little bit of banging my head against a wall, I realised I could just RTFM (or in this case the schema for the KubeadmControlPlane resource), and noticed a convienient `files` array which can be used to add files to the OS on first boot, using cloud-init. This meant I was able to include the kube-vip static pod manifest in `/etc/kubernetes/manifests`, which would then be launched by the kubelet when it started. After I included that, thenode came to life, and I was able to reach the kubernetes API! Scaling up was mostly simple after that, aside from a change in kubernetes 1.29 and up which caused kube-vip to fail unless the initial service account was "super-cluster-admin", but a simple sed command added to the `postKubeadm` array meant that wasn't an issue. 

At this point, I had a full kubernetes cluster, albeit without a CNI installed, and I was ready to Gitops everything.

### Gitops and beyond
Deploying a kubernetes cluster is all well and good, especially if it's automatically provisioned, but I don't appreciate the manual task of running a `kubectl apply` every time I want to do it (which is definitely a lot and not a one time thing). To this end, I decided to deploy FluxCD in order to manage the deployment of both Metal3, and subsequently the kubernetes cluster itself. I first created a directory which contains the base kubeadm cluster resources, and then added a kustomization resource pointing to the `kubernetes/clusters/overlays` directory. In this directory I have a kustomization pointing to the overlay kustomize file. This kustomize file has patches for changing some configuration of the cluster, such as the name (although currently this is done with a Flux postBuild). The overlay kustomize also points to the BaremetalHost manifests themselves. After everything was configured, I pushed my changes to the repo. 

After a while, I saw the first control plane node turn on. At this point I had become quite familiar with the process, so I could recognise by the hard drive LED what stage in the the provisioning process the node had reached. Once I saw that the node was provisioned, I waited for the cloud-init to complete, and the kubernetes API to come up. This process usually takes a while due to kubeadm needing to pull all the images for the kubernetes components. However, after about 10 minutes, the node was still not responding to API requests. After logging SSHing into it, I saw that no containers were running, and cloud-init logs didn't show any signs that it had run the kubeadm initialisation. Without anything else to go by, I assumed that I had screwed up the cluster configuration. I reverted my configuration to the pre flux state, and ran it again, and the cluster deployed successfully. I then made the configuration changes I had done in the kustomize patches, directly in the cluster manifest, and didn't have any issues. Lastly I tried deploying everything by running `kubectl apply -k ./<overlay directory>`, which also succeeded without issue. 

After trying different combinations of this, I was a bit stumped. Then, I saw something in my BareMetalHost manifests: the `userData` field, which was pointing to a cloud-init userData secret I created when I was doing my initial testing. It turns out, each time I would deploy the cluster with Flux, the cluster would deploy, but flux would reconcile and overwrite the host's userData, preventing it from running the cloud-init scripts generated by ClusterAPI. After modifying this setting, I thought I was in the clear, but there was one other thing that kept happening. Everytime I would deploy the cluster with Flux, the nodes would eventually turn themselves off. As I seemingly hadn't learnt my lesson, it took a while to troubleshoot this, but as one may have guessed, it turns out it was just another case of flux reconciling the BareMetalHost. The BaremetalHost resource includes an `online` flag, which tells the baremetal operator whether or not the node should be turned on. I accidently left this field in the "off" state when I deployed the hosts with flux, meaning every time the kustomization would reconcile, the nodes would turn off. I changed this flag, and ran the deployment one last time... I had a kubernetes cluster (again). 

This would have been the end of the journey. I have a fully automated (and declaritive) kubernetes cluster, which I can spin up and down at will, but one thing was still irking me. After deploying a cluster, the nodes wouldn't become Ready, becauseno CNI was being installed. I thought about a couple of ways to solve this. I initially thought I could add a service which ran when the kubelet came up, but having this baked into the image seemed sub optimal. I also thought about using the `postKubeadm` field to install a CNI, but I didn't feel like that would be a particularly elegant solution either. Then, a while later, I was reading the documentation for Flux for an unrelated reason, and saw the `spec.kubeConfig.secretRef` field on the HelmRelease and Kustomization objects. I then remembered that when ClusterAPI deploys a cluster, it generates a secret called "<cluster-name>-kubeconfig" which just contains the whole kubeconfig file. By using this, I was able to create a bootstrap kustomization which depends on the root kustomization resource of the cluster. When the cluster comes up, the bootstrap kustomization deploys HelmReleases which refer to the kubeconfig secret, which in turn deploy the Cilium and Flux helm charts on the remote cluster. 

After implementing this I now had everything. From nodes with no OS, in a powered off state, to a full kubernetes cluster with FluxCD and Cilium running, and all from a single commit in a git repository. 


### But why though
At this point, one would probably be asking themselves why. What possible reason would someone have to do all this. I wish I could say I had some extensive use case, running production workloads, which require highly scalable and available infrastructure, but alas, I will probably just run Jellyfin and the *Arr stack. Instead, I'd say I just did it because it was interesting. I couldn't find anyone who had done something quite like this, so I figured, why not lol. 
